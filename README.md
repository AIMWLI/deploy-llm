### output </br>
xiangyin@DESKTOP-3GKCH15:~$ python3 apply_chat_template.py</br>
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:20<00:00,  5.02s/it]</br>
AutoModelForCausalLM#from_pretrained completed</br>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</br>
 AutoTokenizer#from_pretrained completed</br>
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.</br>
大型语言模型，通常被称为大语言模型（Large Language Model, LLM），是一种机器学习模型，专门用于生成与给定输入相关的自然语言文本。这些模型通过大量的文本数据进行训练，可以理解和生成各种风格和类型的文本，包括但不限于文章、故事、对话、代码等。</br>

在训练过程中，大型语言模型学习了大量语言的规律和结构，从而能够根据提示或指令生成连贯、有意义的文本。它们在多个领域有着广泛的应用，如辅助写作、对话系统、文本摘要、翻译以及创意生成等。近年来，随着计算能力和数据量的提升，大型语言模型的性能得到了显著增强，成为人工智能研究中的重要组成部分。

值得注意的是，尽管大型语言模型在许多方面表现出色，但也存在一些局限性和潜在风险，比如生成不准确的信息、偏见问题以及安全风险等。因此，在应用这些模型时需要谨慎，并采取相应的措施来确保其负责任和道德使用。</br>
xiangyin@DESKTOP-3GKCH15:~$</br>


